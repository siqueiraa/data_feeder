//! Simple performance benchmark runner for optimization validation

use data_feeder::concurrency_optimization::{
    LockFreeRingBuffer, WorkStealingQueue, LoadBalancer, LoadBalancingStrategy
};
use data_feeder::metrics::init_metrics;
use std::sync::{Arc, atomic::{AtomicU64, Ordering}};
use std::time::{Duration, Instant};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("üéØ Simple Performance Benchmark for Multi-Layered Optimizations");
    println!("================================================================");
    
    // Initialize metrics
    let _ = init_metrics();
    
    let operations = 5000;
    let mut all_results = Vec::new();
    
    // Baseline performance test
    println!("\nüìä Running baseline performance test...");
    let baseline_result = run_baseline_test(operations).await;
    println!("‚úÖ Baseline: {:.0} ops/sec", baseline_result.ops_per_second);
    all_results.push(("Baseline".to_string(), baseline_result));
    
    // Lock-free ring buffer test
    println!("\nüìä Running lock-free buffer optimization test...");
    let buffer_result = run_lockfree_buffer_test(operations).await;
    println!("‚úÖ Lock-free Buffer: {:.0} ops/sec", buffer_result.ops_per_second);
    all_results.push(("Lock-free Buffer".to_string(), buffer_result));
    
    // Work-stealing queue test
    println!("\nüìä Running work-stealing queue optimization test...");
    let workstealing_result = run_workstealing_test(operations).await;
    println!("‚úÖ Work-stealing Queue: {:.0} ops/sec", workstealing_result.ops_per_second);
    all_results.push(("Work-stealing Queue".to_string(), workstealing_result));
    
    // Load balancer test
    println!("\nüìä Running load balancer optimization test...");
    let loadbalancer_result = run_loadbalancer_test(operations).await;
    println!("‚úÖ Load Balancer: {:.0} ops/sec", loadbalancer_result.ops_per_second);
    all_results.push(("Load Balancer".to_string(), loadbalancer_result));
    
    // Integrated test
    println!("\nüìä Running integrated optimization test...");
    let integrated_result = run_integrated_test(operations).await;
    println!("‚úÖ Integrated: {:.0} ops/sec", integrated_result.ops_per_second);
    all_results.push(("Integrated".to_string(), integrated_result));
    
    // Generate performance report
    generate_performance_report(&all_results);
    
    Ok(())
}

#[derive(Debug, Clone)]
struct TestResult {
    ops_per_second: f64,
    avg_latency_ms: f64,
    duration: Duration,
}

async fn run_baseline_test(operations: usize) -> TestResult {
    let start = Instant::now();
    let completed = Arc::new(AtomicU64::new(0));
    let total_latency = Arc::new(AtomicU64::new(0));
    
    // Simple sequential processing
    for i in 0..operations {
        let op_start = Instant::now();
        
        // Simulate work
        let mut result = 0u64;
        for j in 0..100 {
            result += (i * j) as u64;
        }
        let _ = result; // Use result
        
        let latency = op_start.elapsed().as_nanos() as u64;
        total_latency.fetch_add(latency, Ordering::Relaxed);
        completed.fetch_add(1, Ordering::Relaxed);
        
        // Occasional yield
        if i % 1000 == 0 {
            tokio::task::yield_now().await;
        }
    }
    
    let duration = start.elapsed();
    let total_ops = completed.load(Ordering::Relaxed) as f64;
    let avg_latency = (total_latency.load(Ordering::Relaxed) as f64 / 1_000_000.0) / total_ops;
    
    TestResult {
        ops_per_second: total_ops / duration.as_secs_f64(),
        avg_latency_ms: avg_latency,
        duration,
    }
}

async fn run_lockfree_buffer_test(operations: usize) -> TestResult {
    let start = Instant::now();
    let buffer = Arc::new(LockFreeRingBuffer::<u64>::new(1000));
    let completed = Arc::new(AtomicU64::new(0));
    let total_latency = Arc::new(AtomicU64::new(0));
    
    // Producer task
    let buffer_clone = buffer.clone();
    let producer = tokio::spawn(async move {
        for i in 0..operations {
            let value = (i * 42) as u64;
            while buffer_clone.try_push(value).is_err() {
                tokio::task::yield_now().await;
            }
        }
    });
    
    // Consumer task
    let buffer_clone = buffer.clone();
    let completed_clone = completed.clone();
    let latency_clone = total_latency.clone();
    let consumer = tokio::spawn(async move {
        while completed_clone.load(Ordering::Relaxed) < operations as u64 {
            let op_start = Instant::now();
            
            if let Some(value) = buffer_clone.try_pop() {
                // Process value
                let _processed = value * 2;
                
                let latency = op_start.elapsed().as_nanos() as u64;
                latency_clone.fetch_add(latency, Ordering::Relaxed);
                completed_clone.fetch_add(1, Ordering::Relaxed);
            } else {
                tokio::task::yield_now().await;
            }
        }
    });
    
    // Wait for completion
    producer.await.unwrap();
    consumer.await.unwrap();
    
    let duration = start.elapsed();
    let total_ops = completed.load(Ordering::Relaxed) as f64;
    let avg_latency = (total_latency.load(Ordering::Relaxed) as f64 / 1_000_000.0) / total_ops;
    
    TestResult {
        ops_per_second: total_ops / duration.as_secs_f64(),
        avg_latency_ms: avg_latency,
        duration,
    }
}

async fn run_workstealing_test(operations: usize) -> TestResult {
    let start = Instant::now();
    let queue = Arc::new(WorkStealingQueue::<u64>::new());
    let completed = Arc::new(AtomicU64::new(0));
    let total_latency = Arc::new(AtomicU64::new(0));
    
    // Fill work queue
    for i in 0..operations {
        queue.push(i as u64).await;
    }
    
    // Worker tasks
    let mut handles = Vec::new();
    for _ in 0..4 {
        let queue_clone = queue.clone();
        let completed_clone = completed.clone();
        let latency_clone = total_latency.clone();
        
        let handle = tokio::spawn(async move {
            while let Some(work_id) = queue_clone.try_pop().await {
                let op_start = Instant::now();
                
                // Process work
                let mut result = 0u64;
                for j in 0..100 {
                    result += work_id * j;
                }
                let _ = result;
                
                let latency = op_start.elapsed().as_nanos() as u64;
                latency_clone.fetch_add(latency, Ordering::Relaxed);
                completed_clone.fetch_add(1, Ordering::Relaxed);
            }
        });
        handles.push(handle);
    }
    
    // Wait for all workers
    for handle in handles {
        handle.await.unwrap();
    }
    
    let duration = start.elapsed();
    let total_ops = completed.load(Ordering::Relaxed) as f64;
    let avg_latency = (total_latency.load(Ordering::Relaxed) as f64 / 1_000_000.0) / total_ops;
    
    TestResult {
        ops_per_second: total_ops / duration.as_secs_f64(),
        avg_latency_ms: avg_latency,
        duration,
    }
}

async fn run_loadbalancer_test(operations: usize) -> TestResult {
    let start = Instant::now();
    let _load_balancer = LoadBalancer::new(LoadBalancingStrategy::LeastLoaded, 4);
    let completed = Arc::new(AtomicU64::new(0));
    let total_latency = Arc::new(AtomicU64::new(0));
    
    // Multiple workers using load balancer
    let mut handles = Vec::new();
    let work_per_worker = operations / 4;
    
    for worker_id in 0..4 {
        let completed_clone = completed.clone();
        let latency_clone = total_latency.clone();
        
        let handle = tokio::spawn(async move {
            let local_balancer = LoadBalancer::new(LoadBalancingStrategy::LeastLoaded, 4);
            
            for i in 0..work_per_worker {
                let op_start = Instant::now();
                
                // Select pool using load balancer
                let pool_id = local_balancer.select_pool(Some(i as u64));
                local_balancer.update_pool_load(pool_id, 1);
                
                // Simulate work based on pool selection
                let mut result = 0u64;
                let work_complexity = 50 + (pool_id * 25);
                for j in 0..work_complexity {
                    result += (worker_id * work_per_worker + i + j) as u64;
                }
                let _ = result;
                
                // Update load after work completion
                local_balancer.update_pool_load(pool_id, -1);
                
                let latency = op_start.elapsed().as_nanos() as u64;
                latency_clone.fetch_add(latency, Ordering::Relaxed);
                completed_clone.fetch_add(1, Ordering::Relaxed);
            }
        });
        handles.push(handle);
    }
    
    // Wait for all workers
    for handle in handles {
        handle.await.unwrap();
    }
    
    let duration = start.elapsed();
    let total_ops = completed.load(Ordering::Relaxed) as f64;
    let avg_latency = (total_latency.load(Ordering::Relaxed) as f64 / 1_000_000.0) / total_ops;
    
    TestResult {
        ops_per_second: total_ops / duration.as_secs_f64(),
        avg_latency_ms: avg_latency,
        duration,
    }
}

async fn run_integrated_test(operations: usize) -> TestResult {
    let start = Instant::now();
    let message_buffer = Arc::new(LockFreeRingBuffer::<String>::new(1000));
    let work_queue = Arc::new(WorkStealingQueue::<usize>::new());
    let results_buffer = Arc::new(LockFreeRingBuffer::<u64>::new(1000));
    let _load_balancer = LoadBalancer::new(LoadBalancingStrategy::AdaptiveHashing, 4);
    
    let completed = Arc::new(AtomicU64::new(0));
    let total_latency = Arc::new(AtomicU64::new(0));
    
    let operations_per_stage = operations / 3;
    
    // Stage 1: Message ingestion
    let buffer_clone = message_buffer.clone();
    let ingestion = tokio::spawn(async move {
        for i in 0..operations_per_stage {
            let message = format!("msg_{}", i);
            while buffer_clone.try_push(message.clone()).is_err() {
                tokio::task::yield_now().await;
            }
        }
    });
    
    // Stage 2: Message to work conversion
    let msg_buffer = message_buffer.clone();
    let work_queue_clone = work_queue.clone();
    let processing = tokio::spawn(async move {
        let mut processed = 0;
        while processed < operations_per_stage {
            if let Some(message) = msg_buffer.try_pop() {
                let work_id = message.len();
                work_queue_clone.push(work_id).await;
                processed += 1;
            } else {
                tokio::task::yield_now().await;
            }
        }
    });
    
    // Stage 3: Work execution with load balancing
    let work_queue_clone = work_queue.clone();
    let results_clone = results_buffer.clone();
    let completed_clone = completed.clone();
    let latency_clone = total_latency.clone();
    
    let mut execution_handles = Vec::new();
    for worker_id in 0..4 {
        let queue_clone = work_queue_clone.clone();
        let results_clone = results_clone.clone();
        let completed_worker = completed_clone.clone();
        let latency_worker = latency_clone.clone();
        
        let handle = tokio::spawn(async move {
            let local_balancer = LoadBalancer::new(LoadBalancingStrategy::AdaptiveHashing, 4);
            
            while let Some(work_id) = queue_clone.try_pop().await {
                let op_start = Instant::now();
                
                // Use load balancer for pool selection
                let _pool_id = local_balancer.select_pool(Some(work_id as u64));
                
                // Perform work
                let mut result = 0u64;
                for i in 0..work_id {
                    result += (i * worker_id + 1) as u64;
                }
                
                // Store result
                while results_clone.try_push(result).is_err() {
                    tokio::task::yield_now().await;
                }
                
                let latency = op_start.elapsed().as_nanos() as u64;
                latency_worker.fetch_add(latency, Ordering::Relaxed);
                completed_worker.fetch_add(1, Ordering::Relaxed);
            }
        });
        execution_handles.push(handle);
    }
    
    // Wait for all stages
    ingestion.await.unwrap();
    processing.await.unwrap();
    for handle in execution_handles {
        handle.await.unwrap();
    }
    
    let duration = start.elapsed();
    let total_ops = completed.load(Ordering::Relaxed) as f64;
    let avg_latency = (total_latency.load(Ordering::Relaxed) as f64 / 1_000_000.0) / total_ops;
    
    // Record integrated metrics (skip to avoid metrics errors)
    // if let Some(metrics) = get_metrics() {
    //     metrics.record_concurrency_performance_summary(
    //         total_ops as u64,
    //         0.95, // High efficiency
    //         0.85, // Good resource utilization
    //     );
    // }
    
    TestResult {
        ops_per_second: total_ops / duration.as_secs_f64(),
        avg_latency_ms: avg_latency,
        duration,
    }
}

fn generate_performance_report(results: &[(String, TestResult)]) {
    println!("\nüéØ PERFORMANCE BENCHMARK RESULTS");
    println!("================================");
    
    let baseline = results.iter().find(|(name, _)| name == "Baseline").map(|(_, result)| result);
    
    for (name, result) in results {
        println!("\nüìä {}", name);
        println!("   Operations/sec: {:.2}", result.ops_per_second);
        println!("   Avg Latency: {:.3}ms", result.avg_latency_ms);
        println!("   Total Duration: {:?}", result.duration);
        
        if let Some(baseline_result) = baseline {
            if name != "Baseline" {
                let improvement = ((result.ops_per_second - baseline_result.ops_per_second) / baseline_result.ops_per_second) * 100.0;
                println!("   Improvement: {:.1}%", improvement);
            }
        }
    }
    
    println!("\nüöÄ PERFORMANCE IMPROVEMENTS");
    println!("=============================");
    
    if let Some((_, baseline_result)) = results.iter().find(|(name, _)| name == "Baseline") {
        let mut targets_met = 0;
        let total_targets = 4;
        
        for (name, result) in results {
            if name == "Baseline" { continue; }
            
            let improvement = ((result.ops_per_second - baseline_result.ops_per_second) / baseline_result.ops_per_second) * 100.0;
            let target = if name.contains("Integrated") { 20.0 } else { 15.0 };
            let meets_target = improvement >= target;
            
            if meets_target {
                targets_met += 1;
            }
            
            println!("üìà {}: {:.1}% improvement {} (Target: {:.0}%+)", 
                     name, improvement, 
                     if meets_target { "‚úÖ MEETS TARGET" } else { "‚ùå BELOW TARGET" }, 
                     target);
        }
        
        println!("\nüìä VALIDATION SUMMARY");
        println!("====================");
        println!("Targets Met: {}/{}", targets_met, total_targets);
        
        if targets_met == total_targets {
            println!("üéâ ALL PERFORMANCE TARGETS MET! Optimization implementation is successful.");
        } else if targets_met >= total_targets / 2 {
            println!("‚ö†Ô∏è PARTIAL SUCCESS: Some targets met, review areas needing improvement.");
        } else {
            println!("‚ùå PERFORMANCE TARGETS NOT MET: Significant optimization work needed.");
        }
    }
    
    println!("\nüí° OPTIMIZATION LAYER ANALYSIS");
    println!("==============================");
    println!("‚Ä¢ Lock-free Buffer: Eliminates mutex contention in high-throughput scenarios");
    println!("‚Ä¢ Work-stealing Queue: Improves load distribution and reduces idle time");
    println!("‚Ä¢ Load Balancer: Optimizes resource allocation across worker pools");
    println!("‚Ä¢ Integrated: Combines all optimizations for maximum performance");
}